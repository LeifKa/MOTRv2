# MOTRv2 Evaluation - Schnellstart

## ðŸš€ Sofort loslegen

### Demo mit synthetischen Daten

```bash
cd /home/es/es_es/es_lekamt00/BeachKI/MOTRv2
python3 analysis/demo_evaluation.py
```

**Output:**
- `analysis/demo_data/plots/` - 5 verschiedene Visualisierungen
- `analysis/demo_data/demo_metrics.json` - Alle Metriken als JSON

---

## ðŸ“‹ Wenn du deine finalen GT-Daten hast

### 1. FÃ¼hre MOTRv2 Inferenz aus

```bash
# Dein Inference-Befehl (Beispiel)
python3 submit_dance.py --exp_name my_experiment --checkpoint path/to/checkpoint.pth
```

### 2. Passe die Pfade in mot_evaluation.py an

Ã–ffne `analysis/mot_evaluation.py` und Ã¤ndere in der `main()` Funktion (Zeile ~767):

```python
# ANPASSEN: Deine Ground Truth JSON
gt_json_path = "data/Dataset/mot/deine_finale_gt.json"

# ANPASSEN: Deine Inference-Ergebnisse
pred_txt_path = "outputs/dein_experiment/tracking_results.txt"
```

### 3. FÃ¼hre die Evaluation aus

```bash
python3 analysis/mot_evaluation.py
```

### 4. Ergebnisse ansehen

```bash
# Plots
ls analysis/plots/

# Metriken
cat analysis/metrics_results.json | python3 -m json.tool
```

---

## ðŸ”¬ Erweiterte Nutzung

### Mehrere Modelle vergleichen

```bash
python3 analysis/example_usage.py 2
```

**Vorher anpassen:** Editiere `example_usage.py` und fÃ¼ge deine Modell-Pfade in `models_to_compare` hinzu (Zeile ~51).

### Verschiedene IoU-Thresholds testen

```bash
python3 analysis/example_usage.py 3
```

### Programmatische Nutzung

```python
from mot_evaluation import MOTEvaluator

# Initialisiere
evaluator = MOTEvaluator(iou_threshold=0.5)

# Lade Daten
evaluator.load_ground_truth_json("path/to/gt.json")
evaluator.load_predictions_mot_format("path/to/predictions.txt")

# Evaluiere
metrics = evaluator.compute_metrics()
losses = evaluator.compute_loss_functions()

# Speichere
evaluator.print_summary()
evaluator.plot_results("output_dir/plots")
evaluator.save_metrics_json("output_dir/metrics.json")

# Zugriff auf Metriken
print(f"MOTA: {metrics['mota']:.4f}")
print(f"F1-Score: {metrics['f1_score']:.4f}")
print(f"Combined Loss: {losses['combined_loss']:.4f}")
```

---

## ðŸ“Š Generierte Visualisierungen

1. **metrics_overview.png**
   - Balkendiagramme fÃ¼r Precision, Recall, F1, IoU, MOTA
   - TP/FP/FN Counts

2. **loss_functions.png**
   - Vergleich aller Loss-Funktionen
   - Color-coded (grÃ¼n=gut, rot=schlecht)

3. **frame_metrics.png**
   - Performance Ã¼ber Zeit
   - GT vs Predicted Detections pro Frame

4. **detection_confusion.png**
   - Confusion-Matrix Style
   - TP, FP, FN Visualisierung

5. **summary_dashboard.png**
   - Umfassendes Dashboard
   - Kombiniert alle wichtigen Metriken

---

## ðŸŽ¯ Metriken-Interpretation

### Gute Werte
- **Precision/Recall/F1**: > 0.8
- **MOTA**: > 0.6
- **Avg IoU**: > 0.7
- **ID Switches**: < 10% der GT Detections

### Akzeptable Werte
- **Precision/Recall/F1**: 0.5 - 0.8
- **MOTA**: 0.3 - 0.6
- **Avg IoU**: 0.5 - 0.7
- **ID Switches**: 10-20%

### Schlechte Werte (Training nÃ¶tig)
- **Precision/Recall/F1**: < 0.5
- **MOTA**: < 0.3
- **Avg IoU**: < 0.5
- **ID Switches**: > 20%

---

## ðŸ”§ Troubleshooting

### "No common frames between GT and predictions"

**Problem:** Frame-IDs stimmen nicht Ã¼berein

**LÃ¶sung:**
1. ÃœberprÃ¼fe die `_extract_frame_id()` Funktion in `mot_evaluation.py`
2. Passe das Regex-Pattern an dein Naming-Schema an
3. Oder: Benenne deine Bilder einheitlich (z.B. `frame_000001.jpg`)

### Plots sehen komisch aus

**LÃ¶sung:**
```python
# In mot_evaluation.py, Ã¤ndere:
plt.rcParams['figure.figsize'] = (20, 15)  # GrÃ¶ÃŸer
plt.savefig(..., dpi=600)  # HÃ¶here AuflÃ¶sung
```

### Speicher-Probleme bei groÃŸen Dateien

**LÃ¶sung:** Verwende einen Subset der Daten
```python
# Nur erste N Frames evaluieren
evaluator.gt_data = dict(list(evaluator.gt_data.items())[:N])
evaluator.pred_data = dict(list(evaluator.pred_data.items())[:N])
```

---

## ðŸ“š Weitere Infos

- **VollstÃ¤ndige Dokumentation:** `analysis/README.md`
- **Beispielcode:** `analysis/example_usage.py`
- **Demo:** `analysis/demo_evaluation.py`

---

## âœ… Checkliste fÃ¼r dein Paper

- [ ] MOTA Score berechnet
- [ ] Precision/Recall/F1 berechnet
- [ ] Visualisierungen erstellt
- [ ] Mit Baseline verglichen (siehe `example_usage.py`)
- [ ] Threshold-SensitivitÃ¤t getestet
- [ ] Metriken als JSON exportiert
- [ ] Plots in LaTeX/Paper eingefÃ¼gt

---

**Viel Erfolg mit deinem Forschungsprojekt! ðŸŽ“**
